{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import scikitplot\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_label_to_text = {\n",
    "  0: 'Angry',\n",
    "  1: 'Fear',\n",
    "  2: 'Happy',\n",
    "  3: 'Neutral',\n",
    "  4: 'Sadness',\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scikitplot (from versions: none)\n",
      "ERROR: No matching distribution found for scikitplot\n"
     ]
    }
   ],
   "source": [
    "!pip install scikitplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6312 images belonging to 5 classes.\n",
      "Found 511 images belonging to 5 classes.\n",
      "Found 251 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dir = \"D:\\\\Facial_emotional_recognition\\\\fer\\\\training\"\n",
    "valid_dir = \"D:\\\\Facial_emotional_recognition\\\\fer\\\\validation\"\n",
    "test_dir = \"D:\\\\Facial_emotional_recognition\\\\fer\\\\test\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                      rotation_range=15,\n",
    "                                      width_shift_range=0.2,\n",
    "                                      height_shift_range=0.2,\n",
    "                                      shear_range=0.2,\n",
    "                                      zoom_range=0.2,\n",
    "                                      horizontal_flip=True)\n",
    "#                                       fill_mode='nearest')\n",
    "\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(48,48),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    class_mode='categorical',\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_dir,\n",
    "                                                    target_size=(48,48),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    class_mode='categorical',\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory( test_dir,\n",
    "                                                    target_size=(48, 48),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "batch_size = 64\n",
    "\n",
    "img_width = 48\n",
    "img_height =48\n",
    "img_depth = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net(optim, num_classes):\n",
    "    \n",
    "    net = Sequential(name='DCNN')\n",
    "    \n",
    "    net.add(Conv2D(filters=64,kernel_size=(5,5),input_shape=(img_width, img_height, img_depth),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d_1'))\n",
    "    net.add(BatchNormalization(name='batchnorm_1'))\n",
    "    \n",
    "    net.add(Conv2D(filters=64, kernel_size=(5,5),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d_2'))\n",
    "    net.add(BatchNormalization(name='batchnorm_2'))    \n",
    "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n",
    "    net.add(Dropout(0.4, name='dropout_1'))\n",
    "\n",
    "    net.add(Conv2D(filters=128,kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d_3'))\n",
    "    net.add(BatchNormalization(name='batchnorm_3'))\n",
    "    net.add( Conv2D(filters=128, kernel_size=(3,3),activation='elu', padding='same',kernel_initializer='he_normal',name='conv2d_4'))\n",
    "    net.add(BatchNormalization(name='batchnorm_4'))   \n",
    "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n",
    "    net.add(Dropout(0.4, name='dropout_2'))\n",
    "\n",
    "    net.add(Conv2D(filters=256, kernel_size=(3,3),activation='elu',padding='same',kernel_initializer='he_normal',name='conv2d_5'))\n",
    "    net.add(BatchNormalization(name='batchnorm_5'))\n",
    "    net.add(Conv2D(filters=256, kernel_size=(3,3), activation='elu',padding='same',kernel_initializer='he_normal', name='conv2d_6'))\n",
    "    net.add(BatchNormalization(name='batchnorm_6'))\n",
    "    \n",
    "    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n",
    "    net.add(Dropout(0.5, name='dropout_3'))\n",
    "\n",
    "    net.add(Flatten(name='flatten'))\n",
    "        \n",
    "    net.add(Dense(128, activation='elu', kernel_initializer='he_normal', name='dense_1'))\n",
    "    net.add(BatchNormalization(name='batchnorm_7'))\n",
    "    \n",
    "    net.add(Dropout(0.6, name='dropout_4'))\n",
    "    \n",
    "    net.add(Dense(num_classes,activation='softmax',name='out_layer'))\n",
    "    \n",
    "    net.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optim,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    net.summary()\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                                min_delta=0.00005,\n",
    "                                patience=11,\n",
    "                                verbose=1,\n",
    "                                restore_best_weights=True)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                factor=0.5,\n",
    "                                patience=7,\n",
    "                                min_lr=1e-7,\n",
    "                                verbose=1)\n",
    "\n",
    "callbacks = [early_stopping,lr_scheduler,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #batch size of 32 performs the best.\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optims = [\n",
    "  optimizers.Nadam(\n",
    "      learning_rate=0.001,\n",
    "      beta_1=0.9,\n",
    "      beta_2=0.999,\n",
    "      epsilon=1e-07,\n",
    "      name='Nadam'),\n",
    "    \n",
    "  optimizers.Adam(0.001),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DCNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        1664      \n",
      "_________________________________________________________________\n",
      "batchnorm_1 (BatchNormalizat (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batchnorm_2 (BatchNormalizat (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "maxpool2d_1 (MaxPooling2D)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batchnorm_3 (BatchNormalizat (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batchnorm_4 (BatchNormalizat (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "maxpool2d_2 (MaxPooling2D)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batchnorm_5 (BatchNormalizat (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batchnorm_6 (BatchNormalizat (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "maxpool2d_3 (MaxPooling2D)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "batchnorm_7 (BatchNormalizat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,395,333\n",
      "Trainable params: 2,393,285\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-142:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 726, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 703, in pool_fn\n",
      "    pool = get_pool_class(True)(\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    return Pool(processes, initializer, initargs, maxtasksperchild,\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\multiprocessing\\pool.py\", line 212, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\multiprocessing\\pool.py\", line 303, in _repopulate_pool\n",
      "    return self._repopulate_pool_static(self._ctx, self.Process,\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\multiprocessing\\pool.py\", line 326, in _repopulate_pool_static\n",
      "    w.start()\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\multiprocessing\\process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\multiprocessing\\context.py\", line 327, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 93, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\Users\\SHIVAM\\anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "TypeError: cannot pickle '_thread.lock' object\n"
     ]
    }
   ],
   "source": [
    "model = build_net(optims[1], num_classes) \n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data = valid_generator,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
